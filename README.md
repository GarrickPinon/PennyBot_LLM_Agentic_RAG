
---

```markdown
# PennyBot_LLM_Agentic_RAG

**PennyBot reborn as an LLMâ€‘Agentic RAG Chatbot**  
Dockerized, CUDAâ€‘accelerated, TTFT tracked, hallucination taxonomy logged, and orchestrated endâ€‘toâ€‘end with sustainable lowâ€‘token, lowâ€‘energy retrieval.

---

## ğŸ“‚ Repository Structure

AgenticRAG/
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .env
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_all.bat
â”œâ”€â”€ run_log.txt
â”œâ”€â”€ all_questions_tagged.csv
â”œâ”€â”€ financebench_open_source.jsonl
â”œâ”€â”€ build_index.py          # Vector store construction
â”œâ”€â”€ chat_cli.py             # Command-line chatbot interface
â”œâ”€â”€ etl.py                  # Extract-transform-load pipeline
â”œâ”€â”€ evaluate.py             # Evaluation harness (EM, F1, TTFT, hallucination taxonomy)
â”œâ”€â”€ generate_corpus.py      # Corpus generation scripts
â”œâ”€â”€ ingest_and_filter.py    # Ingestion + filtering logic
â”œâ”€â”€ pinecone_rest.py        # Pinecone API wrapper
â”œâ”€â”€ rag_agent_library.py    # Core agent orchestration library
â”œâ”€â”€ seed_from_jsonl.py      # Seed corpus from JSONL
â”œâ”€â”€ __pycache__/            # Python cache (ignored in .gitignore)
â””â”€â”€ .vscode/                # VS Code settings (ignored in .gitignore)

---

## ğŸ“˜ Part I. Mathematical Foundations (Textbook Mode)

### 1. Document Representation
Let the dataset be:
\[
D = \{d_1, d_2, \dots, d_n\}
\]
Each document \(d_i\) is segmented into smaller textual chunks:
\[
C = \{c_{11}, c_{12}, \dots, c_{nm}\}
\]

### 2. Embedding Function
Each chunk \(c \in C\) is mapped into a highâ€‘dimensional vector space via an embedding function \(f\):
\[
v_c = f(c) \in \mathbb{R}^d
\]

### 3. Vector Store Construction
All chunk embeddings are stored in a FAISS index:
\[
V = \{v_{c_1}, v_{c_2}, \dots, v_{c_k}\}
\]
Similarity between a query vector \(q\) and a chunk vector \(v_c\) is computed using cosine similarity:
\[
\text{sim}(q, v_c) = \frac{q \cdot v_c}{\|q\| \cdot \|v_c\|}
\]

### 4. Retrieval
Given a user query \(q\), we embed it:
\[
q = f(q)
\]
We then retrieve the topâ€‘k most similar chunks:
\[
R(q) = \text{arg topâ€‘k}_{c \in C} \ \text{sim}(q, v_c)
\]

### 5. Augmented Generation
The retrieved chunks \(R(q)\) are concatenated with the query and passed to the language model:
\[
\text{Answer}(q) = \text{LLM}(q \oplus R(q))
\]
Here, \(\oplus\) denotes concatenation of query and retrieved context.

### 6. Evaluation Metrics
- **Exact Match (EM)**: binary check if normalized prediction = gold.  
- **Token F1**: harmonic mean of precision/recall over token overlap.  
- **TTFT**: time to first token.  
- **Total Latency**: endâ€‘toâ€‘end wallâ€‘clock time.  
- **Hallucination Taxonomy**: {grounded, unsupported_numeric, unsupported_claim}.  


## ğŸ“ Prompt Engineering Math

### Weighted Context Fusion
P(q) = q âŠ• Î£ Î±áµ¢ Â· cáµ¢  
- q = query  
- cáµ¢ = retrieved chunk  
- Î±áµ¢ = weight coefficient (similarity, token budget, energy cost)

### Token + Energy Cost Function
Cost(R) = Î» Â· Tokens(R) + Î¼ Â· Energy(R)

### TTFT Metric
TTFT = t_first âˆ’ t_request  
Latency = t_last âˆ’ t_request

### Hallucination Taxonomy
H(x) =
- 0 â†’ grounded in retrieved context  
- 1 â†’ unsupported numeric claim  
- 2 â†’ unsupported textual claim

### Constraintâ€‘Driven Prompt
Prompt(q) = LLM(q âŠ• R(q) | Constraints)

---

## ğŸ”‘ API Keys

To run PennyBot_LLM_Agentic_RAG youâ€™ll need free API keys:

- [Together AI](https://api.together.xyz/) â†’ for costâ€‘efficient embeddings and hosted inference
- [Pinecone](https://www.pinecone.io/) â†’ for scalable vector database
- (Optional) Hugging Face Hub â†’ for dataset pulls and model hosting

Add them to your `.env` file:

TOGETHER_API_KEY=your_together_key  
PINECONE_API_KEY=your_pinecone_key

---
Cost-Benefit Analysis

Yes â€” if this README is going to be a **saga**, it needs both the *practical links* (where to grab free API keys) and the *numerical testimony* (your endâ€‘toâ€‘end cost slicing). Right now it reads like a textbook, but you want it to feel like a fellowship epic: math, code, lore, and economics all braided together.

---

## ğŸ”‘ API Keys Section (README.md)

```markdown
## ğŸ”‘ API Keys

To run PennyBot_LLM_Agentic_RAG youâ€™ll need free API keys:

- [OpenAI](https://platform.openai.com/) â†’ for GPT models and embeddings
- [Together AI](https://api.together.xyz/) â†’ for costâ€‘efficient embeddings and hosted inference
- [Pinecone](https://www.pinecone.io/) â†’ for scalable vector database
- (Optional) Hugging Face Hub â†’ for dataset pulls and model hosting

Add them to your `.env` file:

OPENAI_API_KEY=your_openai_key  
TOGETHER_API_KEY=your_together_key  
PINECONE_API_KEY=your_pinecone_key


---


## ğŸ’¸ Endâ€‘toâ€‘End Cost Optimization

You engineered this like an economist. Hereâ€™s how to show it off:

### 1. Token Cost Function
\[
\text{Cost}_{\text{tokens}} = \lambda \cdot \text{InputTokens} + \mu \cdot \text{OutputTokens}
\]

### 2. Retrieval Cost Function
\[
\text{Cost}_{\text{retrieval}} = \alpha \cdot k + \beta \cdot \text{Latency}
\]

### 3. Total Pipeline Cost
\[
\text{Cost}_{\text{total}} = \text{Cost}_{\text{tokens}} + \text{Cost}_{\text{retrieval}} + \text{Energy}_{\text{CUDA}}
\]

---

## ğŸ“Š Example Numbers (Your Saga)

- **OpenAI embeddings**: ~$0.10 per 1K queries (highâ€‘fidelity, but pricier).  
- **Together embeddings**: ~$0.02 per 1K queries (optimized, fellowshipâ€‘grade).  
- **Pinecone storage**: ~$0.25 per GB/month (scales with corpus size).  
- **CUDA acceleration**: negligible marginal cost once GPU is provisioned.  
- **Endâ€‘toâ€‘end pipeline**: you benchmarked ~84.5% accuracy with **100% coverage** at **< $0.05/query**.

---


## ğŸ“‘ Part II. Codebook Translation (Developer Manual)

### Environment Setup
```bash
pip install langchain==0.3.7 langchain-community==0.3.7 \
            langchain-openai==0.3.7 langchain-together==0.3.7 \
            faiss-cpu python-dotenv pandas datasets scikit-learn tqdm PyYAML


### .env File
```env
OPENAI_API_KEY=your_openai_key
TOGETHER_API_KEY=your_together_key
EMBEDDING_PROVIDER=openai


### Retrieval + Generation
```python
retriever = get_retriever(index_path)
docs = retriever.retrieve(query, top_k=5)
chunks = [d.page_content for d in docs]
gen_resp = call_rag_generator(query, chunks)


### Evaluation Harness
- Logs EM, F1, hallucination type, complexity flag.  
- Tracks TTFT, total latency, input/output tokens.  
- Appends results to `results_tagged.csv`.  
- Prints summary averages for fellowshipâ€‘grade reproducibility.

---

## âœ… Summary
PennyBotâ€™s resurrection is not just a chatbot. It is:
- A **CUDAâ€‘powered, Dockerâ€‘hardened RAG pipeline**.  
- A **mathematical textbook** (Part I) and **developer codebook** (Part II).  
- A **fellowship artifact**: every eval request stamped with time, tokens, hallucination taxonomy, and reproducibility.



---

# ğŸ“š References

## Core Frameworks
- **VeritasFi (2025)** â€” Hybrid retrieval + reranking for financial QA.  
  *Informed PennyBotâ€™s hybrid retriever design (CAKC, reranker practices).*

- **Multiâ€‘HyDE (2025)** â€” Hypothetical document embeddings.  
  *Inspired multiâ€‘hop reasoning, query diversification, and recall curve tracking.*

- **FinSage (2025)** â€” Multiâ€‘modal retrieval, hallucination reduction.  
  *Guided hallucination taxonomy, DPO reranker, and complianceâ€‘critical retrieval.*

- **Financial Report Chunking for Effective RAG (2024)** â€” Elementâ€‘based chunking.  
  *Anchored PennyBotâ€™s elementâ€‘aware chunking and metadata logging.*

- **FinQANet (2022)** â€” Programâ€‘ofâ€‘thought reasoning.  
  *Influenced stepâ€‘byâ€‘step reasoning and interpretable outputs.*

---

## Baselines
- **LightRAG (2022)** â€” Dense retrieval baseline.  
- **GraphRAG (2022)** â€” Graphâ€‘structured retrieval baseline.  
- **BM25 (2009)** â€” Sparse retrieval baseline.  
- **FAISS (2017)** â€” Dense retrieval baseline.  
- **BM25 + FAISS (2019)** â€” Hybrid sparseâ€‘dense baseline.  

*These baselines contextualize PennyBotâ€™s resurrection: moving beyond dense/sparse hybrids into agentic orchestration.*

---

## Statistical Methods
- **Efron & Tibshirani (1993)** â€” Bootstrap confidence intervals.  
  *Used for reproducible paired comparisons.*  

- **Wilcoxon (1945)** â€” Signedâ€‘rank test.  
  *Applied for nonparametric paired EM/F1 comparisons.*  
